{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Udacity_Bertelsmann_Notebook_6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QctDlezGVAHD",
        "colab_type": "text"
      },
      "source": [
        "# **Saving and loading models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-uSWPLAR6RQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import helper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os97ah0NVOnM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd3MhBvzVVBA",
        "colab_type": "text"
      },
      "source": [
        "# **Train a network**\n",
        "\n",
        "To make things more concise here, I moved the model architecture and training code from the last part to a file called fc_model. Importing this, we can easily create a fully-connected network with fc_model.Network, and train the network using fc_model.train. I'll use this model (once it's trained) to demonstrate how we can save and load models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d59tl1YVesW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 10)\n",
        "        \n",
        "        #With dropout \n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        \n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "        x = self.dropout(F.relu(self.fc4(x)))\n",
        "        \n",
        "        x = F.log_softmax(self.fc5(x), dim=1)\n",
        "        \n",
        "        return x\n",
        "model = Classifier()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ483WGmYVIL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5bef87dc-19ca-406f-f338-37126eada74c"
      },
      "source": [
        "epochs = 2\n",
        "steps = 0\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        log_ps = model(images)\n",
        "        loss = criterion(log_ps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    else:\n",
        "        ## TODO: Implement the validation pass and print out the validation accuracy\n",
        "        test_loss = 0\n",
        "        accuracy = 0\n",
        "        with torch.no_grad():\n",
        "          model.eval()\n",
        "          # validation pass here\n",
        "          for images, labels in testloader:\n",
        "            log_ps = model(images)\n",
        "            test_loss += criterion(log_ps, labels)\n",
        "\n",
        "            ps = torch.exp(log_ps)\n",
        "            top_p, top_class = ps.topk(1, dim=1) \n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "        \n",
        "        model.train()\n",
        "        train_losses.append(running_loss/len(trainloader))\n",
        "        test_losses.append(test_loss/len(testloader))\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/2..  Training Loss: 0.612..  Test Loss: 0.450..  Test Accuracy: 0.839\n",
            "Epoch: 2/2..  Training Loss: 0.442..  Test Loss: 0.420..  Test Accuracy: 0.848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HINA6HJoaxLP",
        "colab_type": "text"
      },
      "source": [
        "# Saving \n",
        "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
        "\n",
        "The parameters for PyTorch networks are stored in a model's state_dict. We can see the state dict contains the weight and bias matrices for each of our layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXWw9-fpavoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "342aacce-5ed0-428d-ed99-14c882416253"
      },
      "source": [
        "print(\"Our model: \\n\\n\", model, '\\n')\n",
        "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our model: \n",
            "\n",
            " Classifier(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc5): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            ") \n",
            "\n",
            "The state dict keys: \n",
            "\n",
            " odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlgzOLCQbQnZ",
        "colab_type": "text"
      },
      "source": [
        "The simplest thing to do is simply save the state dict with torch.save. For example, we can save it to a file 'checkpoint.pth'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPq0OCeYbRfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD6UvpCkbXoI",
        "colab_type": "text"
      },
      "source": [
        "Then we can load the state dict with torch.load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4KaWYLzbYXG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2723ad80-0391-4f6a-a991-5326810fca63"
      },
      "source": [
        "state_dict = torch.load('checkpoint.pth')\n",
        "print(state_dict.keys())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias', 'fc5.weight', 'fc5.bias'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftJcatLvbhaN",
        "colab_type": "text"
      },
      "source": [
        "And to load the state dict in to the network, you do model.load_state_dict(state_dict)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PuOT-oUbmBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76f6a749-773a-4acf-edc6-eb75fc69ce28"
      },
      "source": [
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXrdI3hucTWB",
        "colab_type": "text"
      },
      "source": [
        "Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails.\n",
        "\n",
        "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK3T8LZkcfes",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "54f40c15-4aeb-47c9-d3c0-62c2edff6e0d"
      },
      "source": [
        "checkpoint = {'input_size': 784,\n",
        "              'output_size': 10,\n",
        "              'hidden_layers': [model.fc2, model.fc3, model.fc4],\n",
        "              'state_dict': model.state_dict()}\n",
        "              \n",
        "torch.save(checkpoint, 'checkpoint.pth')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz3LTFyMciYD",
        "colab_type": "text"
      },
      "source": [
        "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bFztBgoclAT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3004c682-47f4-455e-d824-7acb08672cf6"
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = checkpoint\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = load_checkpoint('checkpoint.pth')\n",
        "print(model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_size': 784, 'output_size': 10, 'hidden_layers': [Linear(in_features=512, out_features=256, bias=True), Linear(in_features=256, out_features=128, bias=True), Linear(in_features=128, out_features=64, bias=True)], 'state_dict': OrderedDict([('fc1.weight', tensor([[ 0.0404, -0.0042, -0.0088,  ..., -0.0185,  0.0342,  0.0213],\n",
            "        [ 0.0069,  0.0225, -0.0173,  ..., -0.0057, -0.0225, -0.0320],\n",
            "        [-0.0021, -0.0317, -0.0209,  ..., -0.0484, -0.0542, -0.0057],\n",
            "        ...,\n",
            "        [ 0.0221,  0.0521,  0.0379,  ..., -0.0029,  0.0108,  0.0466],\n",
            "        [-0.0459,  0.0038,  0.0075,  ..., -0.0260, -0.0437, -0.0216],\n",
            "        [-0.0026,  0.0025, -0.0147,  ...,  0.0439,  0.0385,  0.0288]])), ('fc1.bias', tensor([ 7.5682e-03,  2.5104e-02,  3.5547e-02,  9.4171e-03, -3.5796e-02,\n",
            "        -2.9481e-02,  9.7340e-03,  3.3453e-03, -1.7269e-02, -2.9694e-02,\n",
            "         1.9405e-02,  6.3880e-03, -6.7341e-02, -2.5877e-02, -1.7509e-02,\n",
            "         4.5675e-03, -2.1844e-02,  1.9933e-02, -2.6465e-02,  3.9474e-02,\n",
            "        -3.0129e-02, -3.2711e-02, -5.4156e-02, -6.0437e-03, -3.4459e-02,\n",
            "        -4.1498e-02, -2.8639e-02, -2.6938e-02, -4.5326e-02, -4.2011e-02,\n",
            "        -3.1940e-02, -4.5472e-02,  5.7478e-03, -4.1922e-02, -6.8522e-03,\n",
            "        -5.0019e-02,  1.5449e-02, -1.3310e-02, -3.7185e-02,  3.1601e-02,\n",
            "         1.6797e-03, -1.7570e-02, -8.7273e-03,  1.8525e-02,  1.0771e-02,\n",
            "        -8.1044e-03, -1.4349e-02, -3.0612e-02,  3.7606e-02, -3.5503e-02,\n",
            "         2.4158e-02, -2.1925e-02, -1.8958e-02, -3.8746e-02, -5.9095e-02,\n",
            "         1.6236e-03,  3.4391e-03, -3.1976e-02, -2.2159e-03, -2.9902e-03,\n",
            "        -4.7997e-02, -3.2651e-02, -1.8643e-02,  2.1944e-02, -3.7404e-02,\n",
            "         9.1691e-03, -1.5744e-02, -8.2159e-02, -1.4485e-02, -5.2762e-02,\n",
            "        -2.2206e-02, -3.9886e-02, -5.5973e-02,  3.4309e-02, -3.0823e-02,\n",
            "        -2.6800e-02, -3.8217e-02,  3.3516e-02, -2.6444e-02, -2.6969e-02,\n",
            "         1.4267e-02, -2.6312e-02, -5.3321e-02, -2.1967e-02, -5.5485e-02,\n",
            "        -3.7862e-02, -3.3269e-02,  1.2476e-02,  2.2358e-02,  5.6847e-03,\n",
            "        -8.3915e-02, -5.8854e-02, -5.3052e-03, -4.6023e-02,  6.3699e-02,\n",
            "        -4.7417e-02, -3.8836e-03, -3.5532e-02,  1.1875e-02, -1.5949e-03,\n",
            "        -1.8318e-02,  1.2143e-02,  1.1845e-02, -5.8642e-03,  1.0143e-03,\n",
            "        -1.0319e-02, -3.0494e-02,  2.8358e-02, -1.5429e-03,  2.1738e-02,\n",
            "        -4.1739e-02,  2.7691e-02, -1.6775e-02,  4.1834e-02, -8.5045e-02,\n",
            "        -1.7989e-04,  2.8084e-02, -8.8874e-03,  4.4787e-02,  2.2362e-02,\n",
            "         1.0334e-02,  3.3188e-03,  1.0059e-02,  4.8692e-02, -7.0619e-03,\n",
            "        -3.3157e-02,  3.5474e-02,  1.3495e-03,  7.8574e-03,  6.3506e-02,\n",
            "        -2.4555e-03,  1.7194e-02, -7.6643e-03, -3.6270e-02, -1.0886e-02,\n",
            "        -5.1190e-02,  1.1342e-02, -2.9007e-02,  1.2719e-02, -5.4931e-02,\n",
            "         8.6631e-03,  7.5985e-03,  3.6489e-02,  2.1388e-02, -3.7018e-02,\n",
            "        -2.0000e-02, -3.4688e-02, -3.0523e-02, -4.5774e-02, -3.5306e-02,\n",
            "         2.0465e-02,  4.2590e-03,  2.2472e-02, -1.1210e-02, -3.9220e-02,\n",
            "         4.3453e-03, -4.7168e-04,  3.9389e-02, -3.2041e-02, -7.8278e-03,\n",
            "        -1.3397e-02,  3.2318e-02, -6.2894e-02,  1.5978e-02, -1.8871e-02,\n",
            "        -3.8429e-02, -3.1667e-02, -3.6989e-02,  7.0061e-04,  1.2414e-02,\n",
            "         6.8326e-03, -5.9876e-02, -1.8194e-03, -1.8665e-02, -1.7664e-02,\n",
            "        -1.0146e-02,  2.8034e-03, -3.9163e-02, -6.1024e-03, -4.0464e-02,\n",
            "        -3.7954e-04,  8.6884e-04, -3.4931e-02, -3.4981e-02,  2.1446e-02,\n",
            "        -2.0453e-02, -6.5150e-02,  2.5239e-02, -4.8986e-02,  5.4033e-03,\n",
            "         2.1631e-02, -4.1078e-02, -4.1418e-03,  4.6463e-02,  8.0830e-03,\n",
            "        -2.2625e-02, -4.3460e-02,  1.5348e-02, -4.9911e-03, -1.2561e-02,\n",
            "         4.4037e-03, -2.9942e-02, -1.2134e-02, -5.9897e-03, -1.4040e-02,\n",
            "        -2.5725e-02, -2.1668e-03, -2.2241e-02,  4.2119e-03,  2.9334e-02,\n",
            "         3.2116e-02, -2.3964e-02,  8.7338e-03, -5.1576e-02, -5.0332e-03,\n",
            "        -8.4668e-03, -3.4884e-02, -3.3230e-02, -1.6210e-02, -9.6080e-03,\n",
            "        -3.6021e-02, -4.7022e-02, -1.1069e-02, -5.9993e-02, -3.3841e-03,\n",
            "         5.3259e-02, -2.3619e-02,  1.3564e-02,  1.6354e-02, -7.4383e-03,\n",
            "        -2.7303e-02,  1.1695e-02, -4.2859e-02, -3.6948e-02,  9.7790e-03,\n",
            "        -3.6499e-02, -1.4540e-03, -2.8925e-02, -5.0864e-02, -6.2489e-03,\n",
            "        -4.4703e-02, -3.2240e-02,  1.0529e-03, -2.1487e-02,  7.1924e-03,\n",
            "         2.1636e-02, -2.8467e-02,  1.6832e-02, -4.3423e-02,  7.7743e-04,\n",
            "        -7.8809e-03, -2.7990e-02, -2.3120e-02, -1.2555e-02, -3.7609e-02,\n",
            "         5.2147e-03,  6.5736e-02, -4.4040e-02, -3.3831e-02, -3.1415e-02,\n",
            "        -1.6326e-02, -5.8567e-02,  3.2506e-02, -5.1869e-02, -9.5857e-03,\n",
            "        -2.8503e-02,  6.1090e-04, -1.0116e-02, -1.0930e-02, -1.0114e-02,\n",
            "        -1.9878e-02,  8.5277e-03, -1.0369e-02, -5.3280e-02,  5.4935e-03,\n",
            "        -8.3810e-03, -4.8084e-02, -3.3360e-02,  7.9897e-02, -2.4685e-02,\n",
            "        -6.5419e-02,  1.8326e-02, -4.7242e-02, -3.6365e-02, -9.9258e-03,\n",
            "         7.5785e-03, -4.0914e-03,  8.6703e-03, -1.1074e-02, -3.3601e-02,\n",
            "         6.4162e-03, -2.9354e-02, -4.0863e-02,  1.3561e-02, -3.8290e-02,\n",
            "         2.7900e-02,  2.0893e-02, -3.2187e-02, -1.7256e-02, -4.6496e-02,\n",
            "         3.5321e-02,  1.3690e-02,  1.0674e-02,  2.5930e-02,  1.0133e-03,\n",
            "         3.4052e-02, -2.9429e-02, -6.3895e-03, -2.7698e-02, -5.8381e-03,\n",
            "        -6.7653e-03,  1.3325e-02, -4.0899e-02,  7.7838e-03,  1.0373e-05,\n",
            "        -2.5836e-02, -3.3689e-02,  2.4925e-02, -7.0589e-03, -5.2747e-03,\n",
            "        -4.7951e-02, -3.0912e-02, -3.8980e-02,  5.9144e-03,  5.8861e-02,\n",
            "        -4.2977e-02, -3.7387e-02, -1.0276e-02, -4.2504e-02, -5.6252e-02,\n",
            "        -5.0399e-02, -8.0741e-02,  2.2497e-02, -2.3538e-02,  5.2256e-02,\n",
            "         2.3720e-02, -2.7840e-03,  8.2875e-03, -5.0131e-02, -6.5004e-03,\n",
            "        -5.9051e-02,  1.7757e-02,  3.3648e-02, -5.8674e-02, -3.5905e-04,\n",
            "         2.2905e-03, -2.8569e-02, -3.2559e-02, -2.7972e-02, -1.9395e-02,\n",
            "         3.6886e-02, -6.4124e-02, -1.2744e-03, -2.9085e-02,  6.2346e-03,\n",
            "        -4.1450e-03,  4.3309e-03, -6.7720e-02,  9.9358e-04, -1.6317e-02,\n",
            "        -5.7290e-02, -3.9931e-02, -1.5093e-02, -4.3746e-02, -3.8079e-02,\n",
            "        -4.1693e-02, -4.1706e-02, -2.2999e-02, -2.0311e-02, -1.5345e-02,\n",
            "        -4.4350e-02,  1.3577e-04, -1.8447e-02,  1.0858e-02,  1.0165e-03,\n",
            "         1.8420e-03,  1.7340e-02,  9.6392e-03,  4.2247e-03, -1.8549e-02,\n",
            "         3.3389e-02, -1.3622e-02, -1.5276e-02,  3.2971e-02,  2.4509e-02,\n",
            "         2.6920e-02, -6.4863e-02, -2.9369e-02,  1.8335e-03, -4.4137e-02,\n",
            "        -5.5816e-03,  7.8635e-03,  8.3055e-03, -3.9590e-02, -4.3904e-03,\n",
            "        -2.5946e-02, -7.3645e-02,  8.6547e-03, -2.2727e-02, -7.7094e-03,\n",
            "        -9.9630e-03, -1.0165e-02, -5.6110e-02,  9.5899e-03, -5.1741e-02,\n",
            "        -2.9383e-02, -1.0636e-02,  3.4795e-02,  1.6271e-02, -2.4869e-02,\n",
            "        -2.7590e-02,  4.8918e-03, -6.6075e-03, -3.2889e-02,  1.3746e-02,\n",
            "        -2.2092e-02, -3.6279e-02,  1.4256e-02, -2.9187e-02, -8.0173e-03,\n",
            "        -1.9944e-02,  3.0951e-02,  1.2829e-02, -2.8792e-02, -3.6118e-03,\n",
            "        -3.4774e-02, -2.1843e-02,  1.6297e-02, -5.7124e-02, -3.1320e-02,\n",
            "         3.0903e-02, -2.1369e-02,  5.6435e-02, -2.0515e-02, -5.8953e-03,\n",
            "        -1.3778e-02, -5.1788e-02,  1.4241e-02,  1.7432e-02, -4.1225e-02,\n",
            "        -1.2720e-02, -2.5837e-02, -1.8235e-02, -1.4489e-02,  4.0706e-03,\n",
            "        -6.4776e-04, -6.2230e-02, -2.7990e-02, -1.9209e-02,  1.1126e-02,\n",
            "        -3.5025e-02,  4.1083e-02, -5.5441e-02,  2.1601e-03,  9.9345e-04,\n",
            "        -5.9219e-03, -2.5193e-02, -1.8877e-02, -5.1638e-02,  2.3288e-04,\n",
            "         2.4327e-02, -8.9483e-03, -2.2826e-02, -2.7595e-02, -6.2671e-04,\n",
            "         3.9812e-03,  1.1110e-02,  2.7755e-03, -1.9181e-02, -2.5337e-02,\n",
            "        -3.4298e-02, -5.4748e-02, -4.7214e-02,  7.9109e-03, -2.8860e-02,\n",
            "        -2.4981e-02, -1.8412e-02, -4.6963e-03, -2.3191e-02,  7.2657e-03,\n",
            "         4.2370e-02,  2.5407e-02, -2.2848e-02, -5.4860e-02, -2.0240e-02,\n",
            "        -1.8797e-02, -1.8670e-02, -4.7090e-02, -1.0239e-02,  1.8874e-02,\n",
            "         1.1025e-03, -2.8884e-02,  4.3468e-03, -2.7665e-02, -1.5841e-02,\n",
            "        -4.5935e-02, -3.1176e-02, -6.7237e-03,  1.4793e-02, -4.2964e-03,\n",
            "        -1.6935e-02,  4.6268e-03, -5.1344e-02,  1.8886e-02, -2.7398e-02,\n",
            "        -2.7834e-02, -6.2420e-02,  1.9484e-02, -1.4565e-02, -1.4295e-02,\n",
            "         4.2962e-02, -1.5384e-02])), ('fc2.weight', tensor([[ 0.0505, -0.0746,  0.0011,  ..., -0.0825, -0.0185, -0.0105],\n",
            "        [ 0.0763, -0.0124, -0.0069,  ..., -0.0026, -0.1093,  0.0325],\n",
            "        [ 0.0195,  0.0336, -0.0393,  ..., -0.0386,  0.0661, -0.0080],\n",
            "        ...,\n",
            "        [ 0.0230,  0.0905,  0.1073,  ...,  0.0247,  0.0250, -0.0085],\n",
            "        [-0.0199, -0.0191, -0.0465,  ...,  0.0230,  0.0698, -0.0018],\n",
            "        [ 0.0427,  0.0391, -0.1208,  ..., -0.0212, -0.0190,  0.0388]])), ('fc2.bias', tensor([-2.6802e-03,  1.2678e-02,  2.0775e-02, -3.0464e-02,  7.1102e-03,\n",
            "        -4.0543e-02,  5.8261e-02,  8.9066e-03, -2.7472e-03, -4.4613e-02,\n",
            "         1.3749e-02,  2.5374e-03,  3.9138e-02,  4.4287e-02,  2.5302e-02,\n",
            "         2.2052e-02,  5.4566e-03,  9.8723e-03, -3.7895e-02, -4.1334e-02,\n",
            "        -8.2499e-02, -2.9789e-02,  7.4927e-03, -1.4031e-02, -1.8820e-02,\n",
            "         1.6393e-02, -4.2488e-02,  2.5362e-02,  1.4721e-02,  1.0587e-01,\n",
            "        -1.7290e-02, -1.4409e-02,  5.2979e-04,  3.3471e-02,  2.1646e-02,\n",
            "        -2.3361e-02, -6.7803e-03,  5.9165e-02,  5.2671e-02, -3.4651e-02,\n",
            "        -4.9268e-02, -1.8445e-02, -2.7251e-02, -1.7896e-02,  6.5448e-03,\n",
            "        -7.6015e-03, -8.4332e-02, -6.2623e-03,  4.4160e-02,  7.2587e-03,\n",
            "        -4.5464e-02, -6.7991e-04,  8.0647e-02,  1.0763e-02, -4.3168e-02,\n",
            "         9.0100e-03, -4.6786e-02,  3.4469e-02,  6.0833e-03,  8.7632e-02,\n",
            "         1.0677e-02, -6.1344e-04,  1.0545e-02, -1.9505e-02, -5.7659e-04,\n",
            "         2.6285e-02,  4.4926e-02,  4.2798e-02,  6.2320e-02,  1.7793e-02,\n",
            "        -3.2969e-03,  5.2045e-02, -6.1580e-02,  1.1923e-02,  9.5692e-03,\n",
            "         2.9989e-02, -5.3951e-03, -3.2044e-02,  1.6163e-02, -2.0316e-02,\n",
            "        -2.4625e-02, -1.9005e-02,  3.7328e-02,  1.7982e-02,  3.2184e-02,\n",
            "         7.0789e-02,  1.5651e-02,  1.8284e-02, -4.4936e-03,  5.3021e-02,\n",
            "         3.1601e-02, -1.0333e-02,  1.1963e-03,  2.1740e-02,  1.1410e-02,\n",
            "        -7.6701e-05,  3.0847e-02, -3.8440e-03, -5.2217e-02,  4.2078e-02,\n",
            "        -2.3062e-02, -5.8755e-02, -1.3841e-02, -2.0442e-02,  7.1470e-02,\n",
            "        -6.9090e-02, -7.6236e-03, -1.5093e-02, -3.2862e-02,  3.3874e-02,\n",
            "         3.9353e-02,  2.6259e-02,  3.2597e-02, -3.7334e-02, -5.9169e-02,\n",
            "        -1.5682e-02,  3.8284e-02, -6.2454e-02, -3.2345e-02, -4.2469e-02,\n",
            "         2.5548e-03,  2.8551e-02, -2.8842e-02, -2.7996e-02, -5.2775e-03,\n",
            "         2.2954e-02,  2.7235e-02, -1.9025e-02,  7.5097e-03,  4.7006e-02,\n",
            "         2.3690e-02, -1.5809e-02,  7.2354e-02, -4.9912e-02,  7.0375e-02,\n",
            "        -2.0747e-02,  1.3262e-02,  5.5880e-02, -6.7606e-02,  3.7075e-02,\n",
            "         8.3780e-02,  3.2113e-02,  3.0446e-02, -4.7022e-02, -1.6639e-02,\n",
            "         4.1501e-02,  2.4827e-02,  3.5640e-02,  1.2603e-03, -3.4765e-03,\n",
            "         5.0875e-02, -1.5747e-02, -7.4566e-03, -1.0931e-02, -4.3133e-02,\n",
            "        -1.2376e-02, -5.4319e-02,  1.2291e-02,  6.8944e-02,  1.4751e-03,\n",
            "        -2.0681e-02,  2.9948e-02,  1.8424e-02, -1.1046e-03,  4.0321e-02,\n",
            "         4.8954e-02,  5.5364e-02,  5.1802e-02,  3.6614e-02,  2.3561e-02,\n",
            "        -3.7352e-02, -3.1010e-03, -5.7856e-02,  1.2332e-02,  2.5815e-03,\n",
            "         7.1998e-02,  3.8280e-02,  8.7867e-02,  1.9701e-02,  3.4451e-03,\n",
            "        -3.2580e-02,  3.4454e-02,  2.4588e-02, -1.2827e-02,  1.1736e-02,\n",
            "         2.4108e-02,  1.2983e-02, -3.0692e-02,  1.4151e-02,  3.5138e-02,\n",
            "        -4.1212e-02, -5.1573e-02, -4.9436e-02, -7.8216e-02,  5.3800e-02,\n",
            "         1.1396e-02,  1.3950e-02,  7.0834e-02, -3.1228e-02, -2.8070e-02,\n",
            "        -1.7855e-03, -2.3783e-02,  4.8003e-03, -2.6365e-02,  2.0254e-03,\n",
            "         6.5159e-02, -1.9440e-02, -3.4514e-02,  7.8082e-02,  2.0270e-02,\n",
            "        -5.1021e-02, -1.9378e-02,  3.1022e-02,  7.2348e-03,  5.9552e-03,\n",
            "        -3.5814e-02, -8.9189e-03, -2.5210e-02, -4.2660e-02,  3.0932e-02,\n",
            "        -2.9888e-03, -1.8483e-02, -3.8055e-02,  2.4036e-02, -9.9339e-03,\n",
            "         5.3857e-02,  9.9252e-03,  1.1369e-02,  1.0067e-02, -2.0310e-02,\n",
            "         1.6178e-02,  3.3146e-02,  7.1345e-04, -3.9877e-02,  2.2009e-02,\n",
            "         5.3182e-02,  4.3088e-04, -2.7579e-02, -2.4526e-02, -4.1162e-02,\n",
            "         2.8795e-02, -2.0876e-02, -8.8184e-03, -2.9412e-02, -9.7433e-03,\n",
            "        -3.3432e-02, -3.5903e-02,  1.1148e-01,  6.8553e-02, -4.4867e-03,\n",
            "         1.9603e-02,  2.4288e-02,  4.4508e-02, -2.7873e-03,  1.6714e-02,\n",
            "         5.1874e-03])), ('fc3.weight', tensor([[ 0.0292,  0.0172, -0.0865,  ..., -0.0275,  0.0258, -0.0206],\n",
            "        [-0.0326,  0.0261,  0.0129,  ..., -0.0021, -0.0558, -0.0161],\n",
            "        [ 0.0023, -0.0904,  0.0113,  ..., -0.0077,  0.0462, -0.0825],\n",
            "        ...,\n",
            "        [-0.0824, -0.1144, -0.0631,  ..., -0.0131, -0.0065, -0.0260],\n",
            "        [-0.1172,  0.0152, -0.0682,  ..., -0.0028, -0.0455,  0.0520],\n",
            "        [ 0.0190, -0.0113,  0.0424,  ..., -0.0481, -0.0503, -0.0682]])), ('fc3.bias', tensor([ 1.4863e-02,  3.4936e-02,  1.2766e-01, -5.4431e-02,  2.1151e-02,\n",
            "        -1.7138e-02,  5.7638e-02, -1.0485e-02,  4.4871e-02,  6.2550e-02,\n",
            "         1.3944e-01, -3.1955e-02, -8.3408e-02, -3.5567e-02,  1.5094e-02,\n",
            "         2.1731e-02, -5.3609e-02,  2.7839e-02,  4.5568e-02,  7.2751e-02,\n",
            "         7.1802e-02,  4.8177e-02,  1.9573e-02, -7.2609e-03, -1.2796e-02,\n",
            "        -3.6318e-02, -5.4055e-02,  1.1870e-01,  2.8705e-02,  7.1444e-02,\n",
            "         3.5691e-02, -1.0237e-02,  6.1027e-02,  6.6021e-03,  8.2129e-02,\n",
            "         4.9649e-02,  2.5313e-02, -3.5923e-02, -6.5881e-02, -1.1344e-02,\n",
            "        -3.7903e-03,  4.4302e-02,  2.5035e-02,  6.6716e-02,  1.0039e-01,\n",
            "         8.1460e-02, -3.1333e-02,  6.6320e-02, -3.1331e-02, -1.1932e-02,\n",
            "        -3.0062e-02, -1.3768e-02, -2.1417e-02,  3.6119e-03,  3.6884e-02,\n",
            "         4.5869e-02,  7.1385e-02,  2.1636e-02,  5.5999e-02,  1.6758e-03,\n",
            "         1.1374e-01, -1.5088e-03,  2.0743e-02,  4.2113e-02,  4.4712e-02,\n",
            "         1.2492e-02,  5.4111e-02,  7.9985e-02, -3.8938e-02,  6.5641e-03,\n",
            "         6.5132e-02,  7.2934e-02,  2.5245e-02, -3.3431e-02,  1.7603e-02,\n",
            "        -1.1516e-05,  7.7550e-02, -3.2734e-03, -2.6172e-02, -5.0124e-02,\n",
            "         1.8600e-03, -1.9110e-02, -3.6676e-02,  1.1307e-01,  2.6739e-02,\n",
            "         5.1914e-02, -3.2225e-03,  2.8465e-04,  1.1077e-01, -4.9727e-03,\n",
            "        -3.2535e-02,  2.0207e-02, -8.2832e-04, -3.7178e-02,  1.2064e-01,\n",
            "         8.0642e-02,  6.5680e-02, -1.9361e-02,  5.8850e-02,  1.7589e-02,\n",
            "         1.5896e-02,  1.3931e-02, -2.6798e-02,  5.1432e-02, -7.1881e-03,\n",
            "         7.5163e-02,  1.0621e-01, -6.4574e-02,  7.9475e-02,  2.2651e-02,\n",
            "         1.9729e-02,  7.0823e-02,  5.1943e-02,  8.4966e-02, -1.0469e-02,\n",
            "         4.1631e-02, -9.7559e-02,  6.5155e-02, -2.9198e-03,  5.3549e-02,\n",
            "         1.0104e-01,  4.4248e-02, -4.8677e-02,  2.7849e-02,  1.3562e-01,\n",
            "         4.2877e-02,  2.6194e-02,  6.2474e-02])), ('fc4.weight', tensor([[-0.1274, -0.0439, -0.0301,  ...,  0.0053, -0.0853, -0.0781],\n",
            "        [ 0.1160,  0.0143, -0.0968,  ...,  0.0502, -0.0486, -0.0269],\n",
            "        [ 0.0546, -0.1065, -0.0193,  ..., -0.0629, -0.0285,  0.0094],\n",
            "        ...,\n",
            "        [ 0.0684, -0.0699,  0.0448,  ...,  0.0126,  0.0110, -0.0643],\n",
            "        [ 0.0802, -0.0569,  0.0245,  ..., -0.0705,  0.0345,  0.0694],\n",
            "        [-0.0724,  0.1237,  0.0732,  ..., -0.1433, -0.0020,  0.0433]])), ('fc4.bias', tensor([ 0.0488, -0.0207,  0.0347,  0.1530,  0.0417,  0.1734, -0.0670,  0.0043,\n",
            "         0.0367, -0.0123,  0.0701,  0.1230,  0.0183,  0.0221, -0.0223,  0.0876,\n",
            "         0.1277,  0.0304,  0.1460,  0.1385, -0.0567, -0.0441,  0.1196,  0.1343,\n",
            "         0.0026,  0.0133,  0.0630,  0.1540,  0.0145,  0.0586,  0.0097,  0.0801,\n",
            "        -0.0255,  0.0772,  0.0554,  0.1050,  0.1116, -0.0058,  0.1036,  0.0179,\n",
            "         0.0616,  0.0917,  0.0988,  0.0708,  0.0167,  0.0151, -0.0803, -0.0628,\n",
            "        -0.0188,  0.0228,  0.0908,  0.0072,  0.0591,  0.0582,  0.0738,  0.1343,\n",
            "        -0.0056, -0.0336,  0.1564,  0.0225,  0.0843,  0.0107, -0.0872, -0.0530])), ('fc5.weight', tensor([[ 3.9685e-02, -6.6156e-02,  9.4213e-02,  7.8210e-02,  8.3465e-02,\n",
            "          4.8429e-02, -4.5653e-02, -9.6139e-02, -6.2835e-02, -1.6616e-01,\n",
            "          3.5031e-02, -5.7562e-02, -7.0941e-02, -1.0934e-01, -7.1247e-02,\n",
            "         -4.7466e-03,  4.7674e-02,  4.0845e-03, -2.5165e-02, -5.8160e-02,\n",
            "         -2.3039e-02, -4.9388e-02,  2.6195e-02, -8.1907e-02, -7.5771e-02,\n",
            "          4.3112e-04,  1.2463e-01, -9.1210e-02,  7.4161e-02, -6.5604e-02,\n",
            "         -1.2611e-01, -7.6847e-02, -1.3675e-01,  6.8813e-02,  8.2115e-03,\n",
            "         -2.4370e-02,  2.3393e-02,  6.3192e-02, -3.3452e-03, -2.4220e-02,\n",
            "         -1.0845e-01,  8.4298e-02,  8.7146e-02, -1.2333e-01,  8.0612e-02,\n",
            "         -7.6868e-02,  1.1072e-01,  9.0787e-02,  5.0835e-02,  7.5108e-02,\n",
            "         -5.5400e-02,  5.4189e-02, -3.7872e-01,  7.9970e-02, -1.0464e-01,\n",
            "         -8.2110e-02,  9.6790e-02, -5.3603e-02, -5.9916e-04, -1.2016e-01,\n",
            "         -1.0264e-01, -5.5523e-03, -6.8118e-02, -1.1900e-01],\n",
            "        [-6.3278e-02, -5.1783e-02, -6.7999e-02, -8.8683e-02,  1.6795e-02,\n",
            "          3.2923e-02,  8.1163e-02,  6.1889e-02, -6.7990e-02, -7.2788e-02,\n",
            "         -9.7080e-02,  8.4930e-02, -1.3321e-01, -1.5752e-01, -8.3882e-02,\n",
            "         -2.0288e-01,  8.1695e-02,  9.9077e-03, -5.4196e-02, -1.7092e-01,\n",
            "         -2.5753e-02,  7.2184e-02,  7.4418e-02, -1.5338e-01, -7.9717e-03,\n",
            "          6.1651e-02,  1.1937e-02, -1.5448e-01, -4.8147e-02, -1.5002e-01,\n",
            "         -1.4046e-01, -2.3580e-02, -5.6270e-02, -6.9711e-02, -1.0268e-01,\n",
            "         -1.7810e-01, -6.7674e-02, -1.1102e-02, -3.6148e-03, -1.1299e-01,\n",
            "          2.1840e-02, -1.4257e-02, -1.0490e-01, -1.3745e-01, -7.1440e-02,\n",
            "         -5.9009e-02,  1.8441e-02, -5.9562e-02,  1.0669e-01,  1.2175e-01,\n",
            "         -1.2998e-01, -2.2859e-03, -2.7538e-01,  3.3681e-02,  5.4904e-02,\n",
            "         -1.6732e-01, -1.7153e-01, -9.4316e-02, -5.6792e-02,  1.0896e-01,\n",
            "          1.0661e-01, -1.8414e-01,  9.0145e-02,  9.2076e-02],\n",
            "        [-1.6529e-01,  3.2766e-02, -5.5941e-02, -9.9808e-02, -6.4245e-02,\n",
            "          4.5113e-02, -4.4339e-02, -3.0837e-02, -6.8623e-02, -1.8018e-01,\n",
            "          4.8921e-02,  2.8883e-02, -2.6827e-02, -5.2337e-02, -1.7742e-01,\n",
            "          8.9753e-02, -7.1392e-02, -6.8108e-02,  2.3631e-02, -1.7537e-01,\n",
            "         -4.3229e-02, -7.1578e-03, -9.3469e-02, -7.7172e-02, -1.0879e-01,\n",
            "         -1.5618e-01, -7.9029e-02,  4.9011e-02, -4.0572e-02,  4.1135e-03,\n",
            "          9.0247e-02, -7.9269e-02,  8.6212e-02, -3.9093e-02, -2.5352e-02,\n",
            "          6.2139e-03,  7.0922e-02,  2.5236e-02,  8.6625e-02, -6.7334e-02,\n",
            "          7.2399e-02,  8.2867e-02,  6.5286e-02, -8.3239e-02, -5.6298e-02,\n",
            "          2.8317e-02, -1.7906e-02, -4.2063e-02, -9.6947e-02,  8.2897e-02,\n",
            "          9.7912e-04,  7.4860e-02, -3.5202e-01,  6.1471e-02, -1.8450e-01,\n",
            "          2.6556e-02,  7.8136e-02,  1.1512e-01,  1.2049e-01, -6.9829e-02,\n",
            "         -4.8802e-03,  3.9820e-02, -1.1011e-01, -2.4085e-02],\n",
            "        [ 9.4193e-02, -1.6249e-01,  2.9568e-03,  5.6106e-02, -7.5357e-02,\n",
            "          3.1097e-02,  3.7024e-03, -7.2199e-02, -1.6674e-01, -8.6757e-02,\n",
            "         -1.9609e-01,  5.6789e-02, -5.5740e-02, -1.1633e-01,  1.4317e-02,\n",
            "         -6.5354e-02,  8.6933e-02, -1.4166e-01,  5.9129e-02, -9.8126e-02,\n",
            "         -7.4053e-02,  1.0605e-02,  1.0052e-01,  9.2584e-02,  1.2372e-01,\n",
            "          3.4566e-02, -2.1706e-03,  5.0193e-02,  4.6231e-02, -7.4800e-02,\n",
            "         -2.0054e-01,  1.1099e-01, -7.5382e-02,  5.6096e-02, -3.4474e-02,\n",
            "         -4.3108e-02, -1.4062e-01, -4.9214e-02, -5.4603e-02, -1.8611e-01,\n",
            "         -7.7336e-02,  5.5359e-02,  4.3089e-02, -1.2484e-01, -1.2454e-01,\n",
            "         -1.7243e-01,  2.9317e-02, -9.0018e-02,  9.8434e-02,  6.6151e-02,\n",
            "         -1.1859e-02,  8.3662e-02, -1.7818e-01,  1.0096e-01,  4.3244e-02,\n",
            "         -1.3326e-02, -2.7574e-03, -1.7781e-01,  1.0097e-01, -4.9645e-02,\n",
            "         -5.0562e-02, -1.7076e-01, -3.7721e-02,  8.5755e-02],\n",
            "        [-4.3999e-02, -1.1175e-01, -9.5349e-02, -1.1919e-01,  3.6401e-02,\n",
            "         -1.4391e-01, -3.5037e-02,  1.6377e-02, -5.5205e-02, -2.7791e-01,\n",
            "          8.3508e-03,  5.2632e-02, -2.0810e-01,  2.0279e-02, -1.2027e-01,\n",
            "          8.0586e-02,  3.4571e-02,  3.3921e-02, -1.4541e-01, -1.1262e-01,\n",
            "         -2.4522e-01,  4.2525e-02, -5.4254e-02,  3.4772e-02,  9.3852e-02,\n",
            "         -1.1480e-01, -2.3303e-02,  6.5534e-02, -1.8060e-01,  6.9615e-02,\n",
            "          8.9285e-02,  7.6574e-02,  9.7728e-02, -6.4862e-03, -1.7337e-01,\n",
            "          1.8595e-02, -5.5236e-02, -5.1277e-02,  6.5006e-02, -3.6768e-03,\n",
            "          8.1654e-02,  7.4223e-02, -5.5251e-02, -8.5738e-03,  6.2481e-02,\n",
            "          5.4095e-02,  3.1571e-04,  3.3447e-02, -5.4897e-02, -1.6846e-02,\n",
            "          7.7751e-02, -1.1298e-02, -4.6836e-01,  6.5641e-02,  4.8484e-02,\n",
            "          6.7929e-02, -1.9101e-02,  7.4367e-02,  1.1836e-01, -6.0872e-02,\n",
            "         -8.6963e-02,  9.3876e-03, -4.3431e-02,  7.6264e-02],\n",
            "        [-5.7424e-02,  5.0635e-02, -1.6033e-02, -5.0037e-02, -5.4231e-02,\n",
            "         -8.9275e-02,  8.2603e-02,  5.6161e-02,  1.2629e-01,  2.1566e-02,\n",
            "         -3.6758e-01, -3.5049e-01,  6.4227e-02,  5.2496e-02,  6.1041e-02,\n",
            "         -1.6742e-01, -1.2302e-01,  5.3531e-02,  3.8154e-02,  5.9129e-02,\n",
            "          3.1287e-02, -1.7202e-01,  7.3468e-02,  1.2586e-01, -3.9386e-02,\n",
            "          3.3325e-02, -1.0979e-01, -1.7004e-01,  2.4065e-02,  6.4899e-02,\n",
            "          9.8085e-02, -1.9029e-01, -1.7689e-01, -2.1617e-01,  2.9288e-02,\n",
            "         -7.0718e-02,  7.4789e-02, -1.2712e-01,  5.1954e-02,  6.7756e-02,\n",
            "         -2.2070e-01, -2.6628e-01, -8.5475e-02,  7.1498e-02,  6.6506e-02,\n",
            "         -8.1870e-03,  1.0351e-01, -8.8561e-02, -5.7308e-02, -1.0789e-01,\n",
            "         -9.4949e-02,  4.6270e-03, -7.7159e-02, -1.7260e-01, -2.1210e-01,\n",
            "         -1.1661e-01, -1.5807e-01, -2.0048e-01, -1.9491e-01, -6.5327e-02,\n",
            "          8.8632e-02,  4.5426e-02,  2.3939e-02, -6.8722e-03],\n",
            "        [ 8.1329e-03, -4.4153e-02,  8.6791e-02,  2.4942e-02, -9.2160e-03,\n",
            "          1.6786e-02, -8.4829e-02, -3.6943e-02, -1.2954e-01, -1.8137e-01,\n",
            "          3.4824e-02, -9.7756e-02, -5.1482e-02,  2.7721e-02, -1.0292e-01,\n",
            "          7.4795e-02,  2.1950e-02,  6.2892e-02,  1.9700e-02, -1.7600e-01,\n",
            "         -1.2292e-01, -8.5487e-02,  4.2370e-03,  2.8843e-02, -7.1492e-04,\n",
            "         -8.5397e-02,  1.0938e-01,  5.3835e-02,  3.3438e-02, -7.1350e-02,\n",
            "          4.9128e-03,  2.3552e-02,  9.1750e-02,  7.3455e-02, -3.7216e-02,\n",
            "         -6.5125e-02,  5.1708e-02, -6.9530e-02,  6.0229e-02, -4.0435e-02,\n",
            "          1.6760e-02,  5.6627e-02,  9.0129e-02, -3.3564e-02,  8.3345e-02,\n",
            "          8.3086e-02, -1.1368e-02,  9.6806e-02, -4.7187e-02, -2.8225e-02,\n",
            "         -6.9199e-02, -4.4950e-02, -2.4265e-01,  1.5379e-02, -1.0457e-01,\n",
            "         -5.3646e-02,  9.1134e-02, -2.3622e-02,  1.1160e-01, -1.0627e-01,\n",
            "         -6.8225e-02, -2.3903e-02,  1.3882e-03,  4.3587e-02],\n",
            "        [ 6.9298e-02,  5.4959e-02, -1.3866e-02, -1.8504e-01,  5.8452e-02,\n",
            "         -2.1825e-01,  9.4549e-02,  1.2559e-01,  3.5113e-02,  1.1938e-02,\n",
            "         -1.6753e-01, -3.3204e-01, -3.0513e-02, -1.7693e-02,  6.8595e-02,\n",
            "         -3.6764e-01, -2.9786e-01, -1.4735e-01,  8.9126e-02,  6.3847e-02,\n",
            "         -7.6286e-02, -1.4965e-01,  7.6228e-02, -2.7225e-02, -3.4480e-03,\n",
            "         -8.3684e-02,  1.4581e-02, -1.2769e-01,  1.7979e-02,  5.2166e-02,\n",
            "          7.0005e-02, -2.8120e-01, -1.3861e-01, -2.7572e-01,  3.9975e-02,\n",
            "         -1.0358e-01, -1.0686e-01, -5.6633e-02, -1.1444e-01,  9.5684e-02,\n",
            "         -2.4203e-01, -2.5076e-01, -2.4276e-01,  9.2071e-02, -1.1531e-01,\n",
            "         -7.6117e-02, -7.7024e-02, -2.8743e-01,  3.4535e-02, -8.0467e-02,\n",
            "          1.0209e-01, -8.5758e-02,  4.8367e-03, -2.3603e-01, -1.9682e-01,\n",
            "          3.9678e-02, -1.1292e-01,  2.1239e-02, -1.1745e-01,  1.0961e-01,\n",
            "          7.3010e-02,  2.4452e-02,  3.6843e-02,  6.6612e-02],\n",
            "        [-4.2388e-02, -1.4212e-01,  8.2650e-02,  6.4345e-02,  1.4127e-01,\n",
            "         -8.7356e-03, -1.3654e-01, -5.5415e-02,  6.2023e-02, -1.4958e-01,\n",
            "         -8.0608e-02, -1.3040e-01, -1.3351e-01, -5.9040e-02,  4.2464e-02,\n",
            "          3.4246e-02, -9.5338e-02,  5.6395e-02,  8.9168e-02,  4.3999e-02,\n",
            "         -1.1884e-01,  1.6706e-02,  7.5101e-02,  9.9674e-02, -8.1297e-02,\n",
            "         -1.9533e-01, -8.7167e-02, -7.1282e-02, -2.1390e-01,  9.8582e-02,\n",
            "         -1.6426e-02, -1.1471e-01, -4.6498e-02,  2.0580e-02,  6.9169e-02,\n",
            "         -2.2506e-01,  7.4332e-02,  1.0212e-01, -1.6583e-01,  1.3441e-03,\n",
            "         -1.4363e-01, -1.5191e-01, -8.0519e-02,  9.0100e-02,  8.3623e-02,\n",
            "         -2.5348e-03, -9.9834e-02, -2.2603e-01, -1.4441e-01,  1.6953e-02,\n",
            "          6.0395e-02, -1.9510e-01, -1.6478e-01,  1.6507e-02,  3.5940e-02,\n",
            "          2.4149e-02,  5.0900e-02, -3.9905e-03,  6.9020e-02, -3.1073e-02,\n",
            "         -1.1196e-01, -3.2829e-03, -1.8564e-01, -5.3650e-02],\n",
            "        [-5.8803e-02, -5.7289e-02, -1.0180e-01, -1.8628e-01, -6.8814e-02,\n",
            "         -3.3267e-01, -2.2900e-02,  1.2922e-01,  8.7506e-02, -6.9340e-03,\n",
            "         -3.0576e-01, -2.8783e-01,  6.7951e-02, -1.2453e-01,  6.2481e-02,\n",
            "         -2.4258e-01, -2.6232e-01, -1.4645e-01, -8.4027e-02,  7.1555e-02,\n",
            "         -5.1125e-02, -1.1171e-01, -5.7416e-02, -8.9235e-02,  1.0307e-01,\n",
            "          3.9093e-02,  3.5079e-02, -4.6885e-02,  1.9454e-02, -1.3898e-01,\n",
            "          8.0930e-02, -1.1452e-01, -1.2726e-01, -3.0422e-01, -8.0603e-02,\n",
            "         -6.5132e-02,  6.0776e-02, -1.5434e-01,  4.2924e-02, -7.8001e-02,\n",
            "         -1.5052e-01, -3.3469e-01, -1.9635e-01,  1.0749e-01,  8.3874e-02,\n",
            "         -1.8068e-01,  2.0261e-02, -1.3558e-01, -5.8094e-02, -1.2003e-01,\n",
            "          9.5254e-02, -1.4405e-01, -2.0226e-03, -1.7416e-01, -5.5897e-02,\n",
            "          2.6196e-02, -1.6454e-01, -7.9661e-02, -2.4016e-01, -9.2600e-02,\n",
            "         -1.0284e-01,  3.8717e-02,  5.9227e-02, -1.5428e-01]])), ('fc5.bias', tensor([-0.0999, -0.1759,  0.0971,  0.0320, -0.0397, -0.0503, -0.0061,  0.0883,\n",
            "         0.0079,  0.0338]))])}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}