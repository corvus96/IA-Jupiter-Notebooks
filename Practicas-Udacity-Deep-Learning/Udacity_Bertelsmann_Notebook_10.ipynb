{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Udacity_Bertelsmann_Notebook_10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86618a03032845f6b2ac008e89c0f7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a13eea7989794294ad02dc594727494a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_221f7f3d68c4499f971a1ed370439c34",
              "IPY_MODEL_f82a834cd63d473e9ae1b3a67c8883ab"
            ]
          }
        },
        "a13eea7989794294ad02dc594727494a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "221f7f3d68c4499f971a1ed370439c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7567bc60d43742319850eb08b3051c18",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 574673361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574673361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67eea19114624fcca7b8f1052373843a"
          }
        },
        "f82a834cd63d473e9ae1b3a67c8883ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f35a1cfd76744c2e884380a11768031a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [01:24&lt;00:00, 6.78MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_de86123122b04a4aab93d855431c5ae7"
          }
        },
        "7567bc60d43742319850eb08b3051c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67eea19114624fcca7b8f1052373843a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f35a1cfd76744c2e884380a11768031a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "de86123122b04a4aab93d855431c5ae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLNDpA0XPC-0",
        "colab_type": "text"
      },
      "source": [
        "# Style transfer with VGG-19 architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8rO7n-Vr3rl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import resources\n",
        "%matplotlib inline\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7UQSnV9Qb1M",
        "colab_type": "text"
      },
      "source": [
        "# Load in VGG19 (features)\n",
        "\n",
        "VGG19 is split into two portions\n",
        "\n",
        "* vgg19.features, which are all the convolutional and polling layers\n",
        "* vgg19.classifier, which are the three linear classifier layers at the end\n",
        "\n",
        "We only need the features portion, which we're going to load in and \"freeze\" the weights of, below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iTVzMyzROhL",
        "colab_type": "code",
        "outputId": "a021d622-dcde-408a-f958-40c38906c1c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "86618a03032845f6b2ac008e89c0f7f8",
            "a13eea7989794294ad02dc594727494a",
            "221f7f3d68c4499f971a1ed370439c34",
            "f82a834cd63d473e9ae1b3a67c8883ab",
            "7567bc60d43742319850eb08b3051c18",
            "67eea19114624fcca7b8f1052373843a",
            "f35a1cfd76744c2e884380a11768031a",
            "de86123122b04a4aab93d855431c5ae7"
          ]
        }
      },
      "source": [
        "# get the features portion\n",
        "vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "# freeze all VGG parameters since we're only optimizing the target image\n",
        "for param in vgg.parameters():\n",
        "  param.requires_grad_(False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86618a03032845f6b2ac008e89c0f7f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=574673361), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpSnVu9ER58h",
        "colab_type": "code",
        "outputId": "cb9639a1-7279-4023-c9c9-7e63bb982bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "#move the model to GPU, if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "vgg.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (3): ReLU(inplace=True)\n",
              "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (6): ReLU(inplace=True)\n",
              "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (8): ReLU(inplace=True)\n",
              "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (11): ReLU(inplace=True)\n",
              "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (13): ReLU(inplace=True)\n",
              "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (15): ReLU(inplace=True)\n",
              "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (17): ReLU(inplace=True)\n",
              "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (20): ReLU(inplace=True)\n",
              "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (22): ReLU(inplace=True)\n",
              "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (24): ReLU(inplace=True)\n",
              "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (26): ReLU(inplace=True)\n",
              "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (29): ReLU(inplace=True)\n",
              "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (31): ReLU(inplace=True)\n",
              "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (33): ReLU(inplace=True)\n",
              "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (35): ReLU(inplace=True)\n",
              "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO6QZ6BnUfH1",
        "colab_type": "text"
      },
      "source": [
        "# Load in Content and Style images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0yUx9TyUeMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image(img_path, max_size = 400, shape=None):\n",
        "    ''' Load in and transform an image, making sure the image is <= 400 \n",
        "    pixels in the x-y dims.'''\n",
        "\n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # large images will slow down processing\n",
        "    if max(image.size) > max_size:\n",
        "      size = max_size\n",
        "    else:\n",
        "      size = max(image.size)\n",
        "\n",
        "    if shape is not None:\n",
        "      size = shape\n",
        "\n",
        "    in_transform = transforms.Compose([\n",
        "                                       transforms.Resize(size),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                                                            (0.229, 0.224, 0.225))])\n",
        "    # discard the transparent, alpha channel (that's the :3) and add the bath dimension\n",
        "    image = in_transform(image)[:3, :, :].unsqueeze(0)\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03b0057IXraO",
        "colab_type": "text"
      },
      "source": [
        "loading images by file name and forcing the style image to be the same size as the content image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2fNUcdpX8Pg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load in content and style image\n",
        "content = load_image('./content1.jpg').to(device)\n",
        "# Resize style to match content, makes code easier\n",
        "style = load_image('./style3.jpg', shape=content.shape[-2:]).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtA5CDlee-AP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function for un-normalizing an image\n",
        "# and converting it from a tensor to a Numpy image for display\n",
        "def im_convert(tensor):\n",
        "  ''' Display a tensor as an image. '''\n",
        "\n",
        "  image = tensor.to(\"cpu\").clone().detach()\n",
        "  image = image.numpy().squeeze()\n",
        "  image = image.transpose(1,2,0)\n",
        "  image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "  image = image.clip(0, 1)\n",
        "\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTLDZcLfgYA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# display the images\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n",
        "# content and style img side-by-side\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(style))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpf7vc9viAwn",
        "colab_type": "text"
      },
      "source": [
        "# VGG19 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlcSvLCviD38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print out VGG19 structure so you can see the names of various layers\n",
        "print(vgg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynFzbvSwibuF",
        "colab_type": "text"
      },
      "source": [
        "# Content and styles features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4b5yf95igMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(image, model, layers=None):\n",
        "  ''' Run an image forward through a model and get the features for\n",
        "  a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n",
        "  '''\n",
        "\n",
        "  # Need the layers for the content and style representationss of an image\n",
        "  if layers is None:\n",
        "    layers = {'0': 'conv1_1',\n",
        "              '5': 'conv2_1',\n",
        "              '10': 'conv3_1',\n",
        "              '19': 'conv4_1',\n",
        "              '21': 'conv4_2',\n",
        "              '28': 'conv5_1'} # content representation\n",
        "  \n",
        "  features = {}\n",
        "  x = image\n",
        "  # model.modules is a dictionary holding each module in the model\n",
        "  for name, layer in model._modules.items():\n",
        "    x = layer(x)\n",
        "    if name in layers:\n",
        "      features[layers[name]] = x\n",
        "\n",
        "  return features       \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-vcetG4l-gS",
        "colab_type": "text"
      },
      "source": [
        "# Gram Matrix\n",
        "The output of every convolutional layer is a Tensor with dimensions associated with the batch_size, a depth, d and some height and width (h, w). The Gram matrix of a convolutional layer can be calculated as follows:\n",
        "\n",
        "* Get the depth, height and width of a tensor using batch_size, d, h, w = tensor.size\n",
        "* Reshape that tensor so that the spatial dimensions are flattened\n",
        "* Calculate the gram matrix by multiplying the reshaped tensor by it's transpose\n",
        "\n",
        "Note: you can multiply two matrices using torch.mm(matrix1, matrix2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIs6g7CCnM6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(tensor):\n",
        "  ''' Calculate the gram matrix of a given tensor\n",
        "  Gram matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n",
        "  '''\n",
        "  # get the batch size, depth,height, and width of the tensor\n",
        "  _, d, h, w = tensor.size()\n",
        "\n",
        "  # tensor so we're multiplying the features for each channel\n",
        "  tensor = tensor.view(d, h * w)\n",
        "\n",
        "  # calculate the gram matrix\n",
        "  gram = torch.mm(tensor,tensor.t())\n",
        "\n",
        "  return gram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzhaPUiCpBJr",
        "colab_type": "text"
      },
      "source": [
        "# Putting it all Together\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_7zIBDnpFdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get content and style features only once before forming the target image\n",
        "content_features = get_features(content, vgg)\n",
        "style_features = get_features(style, vgg)\n",
        "\n",
        "# calculate the gram matrices for each layer of our style representation\n",
        "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "# create a third 'target' image and prep it for change\n",
        "# it is a good idea to start of with the target as a copy of our content image\n",
        "# then iteratively change its style\n",
        "target = content.clone().requires_grad_(True).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNfIFi8crPza",
        "colab_type": "text"
      },
      "source": [
        "# Loss and weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y33R2hXVrStD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weights for each style layer \n",
        "# weighting earlier layers more will result in 'larger' style artifacts\n",
        "# notice we are excluding 'conv4_2' our content representation\n",
        "style_weights = {'conv1_1': 1.,\n",
        "                 'conv2_1': 0.75,\n",
        "                 'conv3_1': 0.2,\n",
        "                 'conv4_1': 0.2,\n",
        "                 'conv5_1': 0.2}\n",
        "\n",
        "#you may choose to leave these as is\n",
        "content_weight = 1 # alpha\n",
        "style_weight = 1e6 # beta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Jyr90ns621",
        "colab_type": "text"
      },
      "source": [
        "# Updating the target & calculating losses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i75BqDV4tAjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for displaying the target image intermittently\n",
        "show_every = 400\n",
        "\n",
        "# iteration hyperparameters\n",
        "optimizer = optim.Adam([target], lr= 0.03)\n",
        "steps = 2000 # decide how many iterations to update your image (5000)\n",
        "\n",
        "for ii in range(1, steps+1):\n",
        "\n",
        "  # get the features to target image\n",
        "  target_features = get_features(target, vgg)\n",
        "  # the content loss\n",
        "  content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
        "\n",
        "  # the style loss\n",
        "  # initialize the style loss to 0\n",
        "  style_loss = 0\n",
        "  # iterate through each style layer and add to the style loss\n",
        "  for layer in style_weights:\n",
        "    # get the 'target' style representation for the layer\n",
        "    target_feature = target_features[layer]\n",
        "    # Calculate the target gram matrix\n",
        "    target_gram = gram_matrix(target_feature)\n",
        "    _, d, h, w = target_feature.shape\n",
        "    # get the style representation\n",
        "    style_gram = style_grams[layer]\n",
        "    # Calculate the style loss for one layer, wighted appropriately\n",
        "    layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
        "    # add the style loss\n",
        "    style_loss += layer_style_loss/(d * h * w)\n",
        "  \n",
        "  # Calculate the total loss\n",
        "  total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "  \n",
        "  # update your target image\n",
        "  optimizer.zero_grad()\n",
        "  total_loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # display intermediate images and print the loss\n",
        "  if ii % show_every == 0:\n",
        "    print('Total loss: ', total_loss.item())\n",
        "    plt.imshow(im_convert(target))\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N17NA_DulHY",
        "colab_type": "text"
      },
      "source": [
        "# Display the target image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5wPJzXU1805",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# display content and final target image\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(target))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}