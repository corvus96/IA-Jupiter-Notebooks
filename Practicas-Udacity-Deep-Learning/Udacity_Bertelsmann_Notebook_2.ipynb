{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Udacity-Bertelsmann-Notebook-2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZgy4ZwiCj-U",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "##Gradient descent\n",
        "---\n",
        "In this lab, we'll implement the basic functions of the Gradient Descent algorithm to find the boundary in a small dataset. First, we'll start with some functions that will help us plot and visualize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG4xRM24CU-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Some helper functions for plotting and drawing lines\n",
        "\n",
        "def plot_points(X, y):\n",
        "    admitted = X[np.argwhere(y==1)]\n",
        "    rejected = X[np.argwhere(y==0)]\n",
        "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')\n",
        "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')\n",
        "\n",
        "def display(m, b, color='g--'):\n",
        "    plt.xlim(-0.05,1.05)\n",
        "    plt.ylim(-0.05,1.05)\n",
        "    x = np.arange(-10, 10, 0.1)\n",
        "    plt.plot(x, m*x+b, color)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXw1C_AFDBmZ",
        "colab_type": "text"
      },
      "source": [
        "## Reading and plotting the data\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djN8oYenC-Wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('data.csv', header=None)\n",
        "X = np.array(data[[0,1]])\n",
        "y = np.array(data[2])\n",
        "plot_points(X,y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYQM2Hm1DZb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Activation (sigmoid) function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Output (prediction) formula\n",
        "def output_formula(features, weights, bias):\n",
        "    return sigmoid(np.dot(features,weights) + bias)\n",
        "\n",
        "# Error (log-loss) formula\n",
        "def error_formula(y, output):\n",
        "    return -y * np.log(output) - (1 - y) * np.log(1-output)\n",
        "\n",
        "# Gradient descent step\n",
        "def update_weights(x, y, weights, bias, learnrate):\n",
        "    output = output_formula(x, weights, bias)\n",
        "    weights -= learnrate * ( - (- y - output) * x )\n",
        "    bias += learnrate * ( y - output)\n",
        "    return weights,bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BajD6_VDiZA",
        "colab_type": "text"
      },
      "source": [
        "# Training function\n",
        "---\n",
        "This function will help us iterate the gradient descent algorithm through all the data, for a number of epochs. It will also plot the data, and some of the boundary lines obtained as we run the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5m3HWXzDf54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(44)\n",
        "\n",
        "epochs = 26\n",
        "learnrate = 0.01\n",
        "\n",
        "def train(features, targets, epochs, learnrate, graph_lines=False):\n",
        "    \n",
        "    errors = []\n",
        "    n_records, n_features = features.shape\n",
        "    last_loss = None\n",
        "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "    bias = 0\n",
        "    for e in range(epochs):\n",
        "        del_w = np.zeros(weights.shape)\n",
        "        for x, y in zip(features, targets):\n",
        "            output = output_formula(x, weights, bias)\n",
        "            error = error_formula(y, output)\n",
        "            weights, bias = update_weights(x, y, weights, bias, learnrate)\n",
        "        \n",
        "        # Printing out the log-loss error on the training set\n",
        "        out = output_formula(features, weights, bias)\n",
        "        loss = np.mean(error_formula(targets, out))\n",
        "        errors.append(loss)\n",
        "        if e % (epochs / 10) == 0:\n",
        "            print(\"\\n========== Epoch\", e,\"==========\")\n",
        "            if last_loss and last_loss < loss:\n",
        "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "            else:\n",
        "                print(\"Train loss: \", loss)\n",
        "            last_loss = loss\n",
        "            predictions = out > 0.5\n",
        "            accuracy = np.mean(predictions == targets)\n",
        "            print(\"Accuracy: \", accuracy)\n",
        "        if graph_lines and e % (epochs / 100) == 0:\n",
        "            display(-weights[0]/weights[1], -bias/weights[1])\n",
        "            \n",
        "\n",
        "    # Plotting the solution boundary\n",
        "    plt.title(\"Solution boundary\")\n",
        "    display(-weights[0]/weights[1], -bias/weights[1], 'black')\n",
        "\n",
        "    # Plotting the data\n",
        "    plot_points(features, targets)\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting the error\n",
        "    plt.title(\"Error Plot\")\n",
        "    plt.xlabel('Number of epochs')\n",
        "    plt.ylabel('Error')\n",
        "    plt.plot(errors)\n",
        "    plt.show()\n",
        "\n",
        "    # Min error\n",
        "    print(\"El minimo error se encuentra:\")\n",
        "    print(min(errors))\n",
        "    print(np.argwhere(errors == min(errors)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwK-APh9Dvog",
        "colab_type": "text"
      },
      "source": [
        "# Time to train the algorithm!\n",
        "---\n",
        "When we run the function, we'll obtain the following:\n",
        "\n",
        "* 10 updates with the current training loss and accuracy\n",
        "* A plot of the data and some of the boundary lines obtained. The final one is in black. Notice how the lines get closer and closer to the best fit, as we go through more epochs.\n",
        "* A plot of the error function. Notice how it decreases as we go through more epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAFtTyxFEL_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(X, y, epochs, learnrate, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZSaVQN1rKpM",
        "colab_type": "text"
      },
      "source": [
        "# Practice (Student admissions)\n",
        "---\n",
        "\n",
        "\n",
        "*   GRE Scores (Test)\n",
        "*   GPA Scores (Grades)\n",
        "*   Class rank (1-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIPQQTovrJgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the csv file into a pandas DataFrame\n",
        "data = pd.read_csv('student_data.csv')\n",
        "# Printing out the first 10 rows of our data\n",
        "data[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psey626ztXb8",
        "colab_type": "text"
      },
      "source": [
        "# Plotting the data\n",
        "---\n",
        "First let's make a plot of our data to see how it looks. In order to have a 2D plot, let's ingore the rank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XekpthHttoxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to help us plot\n",
        "def plot_points(data):\n",
        "    X = np.array(data[[\"gre\",\"gpa\"]])\n",
        "    y = np.array(data[\"admit\"])\n",
        "    admitted = X[np.argwhere(y==1)]\n",
        "    rejected = X[np.argwhere(y==0)]\n",
        "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'red', edgecolor = 'k')\n",
        "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')\n",
        "    plt.xlabel('Test (GRE)')\n",
        "    plt.ylabel('Grades (GPA)')\n",
        "    \n",
        "# Plotting the points\n",
        "plot_points(data)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUoLbOslt7D7",
        "colab_type": "text"
      },
      "source": [
        "Roughly, it looks like the students with high scores in the grades and test passed, while the ones with low scores didn't, but the data is not as nicely separable as we hoped it would. Maybe it would help to take the rank into account? Let's make 4 plots, each one for each rank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPbeRrwIt8eX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separating the ranks\n",
        "data_rank1 = data[data[\"rank\"]==1]\n",
        "data_rank2 = data[data[\"rank\"]==2]\n",
        "data_rank3 = data[data[\"rank\"]==3]\n",
        "data_rank4 = data[data[\"rank\"]==4]\n",
        "\n",
        "# Plotting the graphs\n",
        "plot_points(data_rank1)\n",
        "plt.title(\"Rank 1\")\n",
        "plt.show()\n",
        "plot_points(data_rank2)\n",
        "plt.title(\"Rank 2\")\n",
        "plt.show()\n",
        "plot_points(data_rank3)\n",
        "plt.title(\"Rank 3\")\n",
        "plt.show()\n",
        "plot_points(data_rank4)\n",
        "plt.title(\"Rank 4\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCxH6hJ2uwIW",
        "colab_type": "text"
      },
      "source": [
        "This looks more promising, as it seems that the lower the rank, the higher the acceptance rate. Let's use the rank as one of our inputs. In order to do this, we should one-hot encode it.\n",
        "\n",
        "# TODO: One-hot encoding the rank\n",
        "Use the `get_dummies` function in Pandas in order to one-hot encode the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncGSWO0AuuXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO:  Make dummy variables for rank\n",
        "one_hot_data = pd.concat([data, pd.get_dummies(data[\"rank\"], prefix = \"rank\")], axis = 1)\n",
        "\n",
        "# TODO: Drop the previous rank column\n",
        "one_hot_data = one_hot_data.drop('rank', axis=1)\n",
        "\n",
        "# Print the first 10 rows of our data\n",
        "one_hot_data[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XuBTHhRvJkv",
        "colab_type": "text"
      },
      "source": [
        "# TODO:  Scaling the data\n",
        "\n",
        "The next step is to scale the data. We notice that the range for grades is 1.0-4.0, whereas the range for test scores is roughly 200-800, which is much larger. This means our data is skewed, and that makes it hard for a neural network to handle. Let's fit our two features into a range of 0-1, by dividing the grades by 4.0, and the test score by 800."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1tg_FFYvRJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making a copy of our data\n",
        "processed_data = one_hot_data[:]\n",
        "\n",
        "# TODO: Scale the columns\n",
        "processed_data[\"gre\"] = processed_data[\"gre\"]/800\n",
        "processed_data[\"gpa\"] = processed_data[\"gpa\"]/4.0\n",
        "# Printing the first 10 rows of our procesed data\n",
        "processed_data[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecOnj5mbvdb2",
        "colab_type": "text"
      },
      "source": [
        "# Splitting the data into Training and Testing\n",
        "\n",
        "In order to test our algorithm, we'll split the data into a Training and a Testing set. The size of the testing set will be 10% of the total data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PINIT2aTvp4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = np.random.choice(processed_data.index, size=int(len(processed_data)*0.9), replace=False)\n",
        "train_data, test_data = processed_data.iloc[sample], processed_data.drop(sample) # Method iloc() find the row with the index\n",
        "\n",
        "print(\"Number of training samples is\", len(train_data))\n",
        "print(\"Number of testing samples is\", len(test_data))\n",
        "print(train_data[:10])\n",
        "print(test_data[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgNF1QAlvuOM",
        "colab_type": "text"
      },
      "source": [
        "# Splitting the data into features and targets (labels)\n",
        "\n",
        "Now, as a final step before the training, we'll split the data into features (X) and targets (y)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHCmynabv2Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = train_data.drop('admit', axis=1)\n",
        "targets = train_data['admit']\n",
        "features_test = test_data.drop('admit', axis=1)\n",
        "targets_test = test_data['admit']\n",
        "\n",
        "print(features[:10])\n",
        "print(targets[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pSp5VL8v6Cj",
        "colab_type": "text"
      },
      "source": [
        "# Training the 2-layer Neural Network\n",
        "The following function trains the 2-layer neural network. First, we'll write some helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEx0mZcuwBvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Activation (sigmoid) function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1-sigmoid(x))\n",
        "def error_formula(y, output):\n",
        "    return - y*np.log(output) - (1 - y) * np.log(1-output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo8d0FQ3wFeb",
        "colab_type": "text"
      },
      "source": [
        "TODO: Backpropagate the error\n",
        "Now it's your turn to shine. Write the error term. Remember that this is given by the equation\n",
        "\n",
        "$$ (y- \\hat{y})\\sigma\\prime(x)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIcgCN9A4wkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Write the error term formula\n",
        "def error_term_formula(x, y, output):\n",
        "    return (y - output) * sigmoid_prime(x)\n",
        "## Alternative solution ##\n",
        "# you could also *only* use y and the output \n",
        "# and calculate sigmoid_prime directly from the activated output!\n",
        "\n",
        "# below is an equally valid solution (it doesn't utilize x)\n",
        "def error_term_formula_1(x, y, output):\n",
        "    return (y-output) * output * (1 - output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdubLUMv46OD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural Network hyperparameters\n",
        "epochs = 1000\n",
        "learnrate = 0.5\n",
        "\n",
        "# Training function\n",
        "def train_nn(features, targets, epochs, learnrate):\n",
        "    \n",
        "    # Use to same seed to make debugging easier\n",
        "    np.random.seed(42)\n",
        "\n",
        "    n_records, n_features = features.shape\n",
        "    last_loss = None\n",
        "\n",
        "    # Initialize weights\n",
        "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "\n",
        "    for e in range(epochs):\n",
        "        del_w = np.zeros(weights.shape)\n",
        "        for x, y in zip(features.values, targets):\n",
        "            # Loop through all records, x is the input, y is the target\n",
        "\n",
        "            # Activation of the output unit\n",
        "            #   Notice we multiply the inputs and the weights here \n",
        "            #   rather than storing h as a separate variable \n",
        "            output = sigmoid(np.dot(x, weights))\n",
        "\n",
        "            # The error, the target minus the network output\n",
        "            error = error_formula(y, output)\n",
        "\n",
        "            # The error term\n",
        "            error_term = error_term_formula(x, y, output)\n",
        "\n",
        "            # The gradient descent step, the error times the gradient times the inputs\n",
        "            del_w += error_term * x\n",
        "\n",
        "        # Update the weights here. The learning rate times the \n",
        "        # change in weights, divided by the number of records to average\n",
        "        weights += learnrate * del_w / n_records\n",
        "\n",
        "        # Printing out the mean square error on the training set\n",
        "        if e % (epochs / 10) == 0:\n",
        "            out = sigmoid(np.dot(features, weights))\n",
        "            loss = np.mean((out - targets) ** 2)\n",
        "            print(\"Epoch:\", e)\n",
        "            if last_loss and last_loss < loss:\n",
        "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "            else:\n",
        "                print(\"Train loss: \", loss)\n",
        "            last_loss = loss\n",
        "            print(\"=========\")\n",
        "    print(\"Finished training!\")\n",
        "    return weights\n",
        "    \n",
        "weights = train_nn(features, targets, epochs, learnrate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XyQx5fN5S6N",
        "colab_type": "text"
      },
      "source": [
        "# Calculating the Accuracy on the Test Data\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eUw1IRL5L9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate accuracy on test data\n",
        "test_out = sigmoid(np.dot(features_test, weights))\n",
        "predictions = test_out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}